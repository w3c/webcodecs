<pre class='metadata'>
Title: WebCodecs
Repository: wicg/web-codecs
Status: CG-DRAFT
ED: https://wicg.github.io/web-codecs/
Shortname: web-codecs
Level: 1
Group: wicg
Editor: Chris Cunningham, w3cid 114832, Google Inc. https://google.com/
Editor: Paul Adenot, w3cid 62410, Mozilla https://www.mozilla.org/

Abstract: This specification defines interfaces for encoding and decoding audio
Abstract: and video. It also includes an interface for retrieving raw video
Abstract: frames from MediaStreamStracks.

Markup Shorthands:css no, markdown yes, dfn yes
!Participate: <a href="https://github.com/wicg/web-codecs">Git Repository.</a>
!Participate: <a href="https://github.com/wicg/web-codecs/issues/new">File an issue.</a>
!Version History: <a href="https://github.com/wicg/web-codecs/commits">https://github.com/wicg/web-codecs/commits</a>
</pre>

<pre class='anchors'>
spec: media-source; urlPrefix: https://www.w3.org/TR/media-source/
    type: method
        for: MediaSource; text: isTypeSupported(); url: #dom-mediasource-istypesupported

spec: html; urlPrefix: https://html.spec.whatwg.org/multipage/;
    type: method
        for: HTMLMediaElement; text: canPlayType(); url: #dom-navigator-canplaytype
</pre>


Definitions {#definitions}
==========================

: Codec
:: Refers generically to the types: AudioDecoder, AudioEncoder, VideoDecoder,
    and VideoEncoder.

: Key Frame
:: An encoded frame that does not depend on any other frames for decoding.


Processing Model {#processing-model}
====================================

New codec tasks may be scheduled while previous tasks are still pending. For
example, web authors may call `decode()` without waiting for the previous
`decode()` to generate an output. This is facilitated by the following
mechanisms.

Each codec has a single <dfn>control message queue</dfn> that is a list of
<dfn>control messages</dfn>.

<dfn lt="Enqueues a control message|Queue a control message">Queuing a control message</dfn>
    means adding the message to the end of a codec’s <a>control message queue</a>.
    Invoking codec methods will often queue a control message to schedule work.

<dfn lt="running a control message|control message steps">Running a control message</dfn> means executing
a sequence of steps specified by the method that enqueued the message.

Control messages in a control message queue are ordered by time of insertion.
The oldest message is therefore the one at the front of the control message
queue.

<dfn lf="run the control message processing loop">Running the control message processing loop</dfn> means executing these
steps.
1. While the control message queue is not empty
    1. Let |front message| be the next oldest <a>control message</a>
    2. If |front message| cannot be executed now, return.

        The User Agent must decide when further processing is blocked because of
            ongoing work as an implementation detail (e.g. the underlying
            decoder cannot accept more requests yet). The UA must restart the
            processing loop when the blockage is resolved.

        NOTE: a blocked processing loop is visible to authors via the
            decodeQueueSize and encodeQueueSize attributes.

    3. Dequeue |front message| from the <a>control message queue</a>.
    4. Run the |front message| <a>control message steps</a>.


AudioDecoder Interface {#audiodecoder-interface}
================================================

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface AudioDecoder {
  constructor(AudioDecoderInit init);

  readonly attribute CodecState state;
  readonly attribute long decodeQueueSize;

  void configure(AudioDecoderConfig config);
  void decode(EncodedAudioChunk chunk);
  Promise<void> flush();
  void reset();
  void close();
};

dictionary AudioDecoderInit {
  required AudioFrameOutputCallback output;
  required WebCodecsErrorCallback error;
};

callback AudioFrameOutputCallback = void(AudioFrame output);
</xmp>
</pre>

Internal Slots {#audiodecoder-internal-slots}
---------------------------------------------
<dl>
<dt><dfn for=AudoDecoder>[[codec implementation]]</dfn></dt>
<dd>Underlying decoder implementation provided by the User Agent.</dd>
<dt><dfn for=AudoDecoder>[[output callback]]</dfn></dt>
<dd>Callback given at construction for decoded outputs.</dd>
<dt><dfn for=AudoDecoder>[[error callback]]</dfn></dt>
<dd>Callback given at construction for decode errors.</dd>
</dl>

Constructors {#audiodecoder-constructors}
-----------------------------------------
<dfn constructor for=AudioDecoder title="AudioDecoder(init)">
  AudioDecoder(init)
</dfn>
1. Let d be a new AudioDecoder object.
2. Assign init.output to the [[output callback]] internal slot.
3. Assign init.error to the [[error callback]] internal slot.
4. Assign "unconfigured" to d.state.
4. Return d.

Attributes {#audiodecoder-attributes}
-------------------------------------
<dl>
  <dt>
    <dfn attribute for=AudioDecoder>state</dfn>
  </dt>
  <dd>Describes the current state of the codec.</dd>
  <dt>
    <dfn attribute for=AudioDecoder>decodeQueueSize</dfn>
  </dt>
  <dd>
    The number of pending decode requests. This does not include requests that
    have been sent to the underlying codec.
  </dd>
</dl>

Methods {#audiodecoder-methods}
-------------------------------
<dl>
  <dt><dfn method for=AudioDecoder>configure(config)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to configure the audio decoder for
    decoding chunks as described by |config|.

    When invoked, run these steps:
    1. If |config| is not a <a>valid AudioDecoderConfig</a>, throw a
        {{TypeError}}.
    2. Run the <a>Configure Decoder</a> algorithm with |config|.
  </dd>

  <dt><dfn method for=AudioDecoder>decode(chunk)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to decode the given |chunk|.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>AudioFrame Output</a> algorithm.
    2. Run the <a>Decode Chunk</a> algorithm with |chunk| and
        |output algorithm|.
  </dd>

  <dt><dfn method for=AudioDecoder>flush()</dfn></dt>
  <dd>
    Completes all <a>control messages</a> in the <a>control message queue</a>
    and emits all outputs.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>AudioFrame Output</a> algorithm.
    2. Run the <a>Flush</a> algorithm with |output algorithm|.
  </dd>

  <dt><dfn method for=AudioDecoder>reset()</dfn></dt>
  <dd>
    Immediately resets all state including configuration,
    <a>control messages</a> in the <a>control message queue</a>, and all pending
    callbacks.

    When invoked, run the <a>Reset</a> algorithm.
  </dd>

  <dt><df method for=AudioDecoder>close()</df></dt>
  <dd>
    Immediately aborts all pending work and releases system resources. Close is
    permanent.

    When invoked, run the <a>Close</a> algorithm.
  </dd>
</dl>

VideoDecoder Interface {#videodecoder-interface}
================================================

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface VideoDecoder {
  constructor(VideoDecoderInit init);

  readonly attribute CodecState state;
  readonly attribute long decodeQueueSize;

  void configure(VideoDecoderConfig config);
  void decode(EncodedVideoChunk chunk);
  Promise<void> flush();
  void reset();
  void close();
};

dictionary VideoDecoderInit {
  required VideoFrameOutputCallback output;
  required WebCodecsErrorCallback error;
};

callback VideoFrameOutputCallback = void(VideoFrame output);
</xmp>
</pre>

Internal Slots {#videodecoder-internal-slots}
---------------------------------------------
<dl>
<dt><dfn for=VideoDecoder>[[codec implementation]]</dfn></dt>
<dd>Underlying decoder implementation provided by the User Agent.</dd>
<dt><dfn for=VideoDecoder>[[output callback]]</dfn></dt>
<dd>Callback given at construction for decoded outputs.</dd>
<dt><dfn for=VideoDecoder>[[error callback]]</dfn></dt>
<dd>Callback given at construction for decode errors.</dd>
</dl>

Constructors {#videodecoder-constructors}
-----------------------------------------
<dfn constructor for=VideoDecoder title="VideoDecoder(init)">
  VideoDecoder(init)
</dfn>
1. Let d be a new VideoDecoder object.
2. Assign init.output to the [[output callback]] internal slot.
3. Assign init.error to the [[error callback]] internal slot.
4. Assign "unconfigured" to d.state.
5. Return d.

Attributes {#videodecoder-attributes}
-------------------------------------
<dl>
  <dt>
    <dfn attribute for=VideoDecoder>state</dfn>
  </dt>
  <dd>Describes the current state of the codec.</dd>
  <dt>
    <dfn attribute for=VideoDecoder>decodeQueueSize</dfn>
  </dt>
  <dd>
    The number of pending decode requests. This does not include requests that
    have been sent to the underlying codec.
  </dd>
</dl>

Methods {#videodecoder-methods}
-------------------------------
<dl>
  <dt><dfn method for=VideoDecoder>configure(config)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to configure the video decoder for
    decoding chunks as described by |config|.

    When invoked, run these steps:
    1. If |config| is not a <a>valid VideoDecoderConfig</a>, throw a
        {{TypeError}}.
    2. Run the <a>Configure Decoder</a> algorithm with |config|.
  </dd>

  <dt><dfn method for=VideoDecoder>decode(chunk)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to decode the given |chunk|.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>VideoFrame Output</a> algorithm.
    2. Run the <a>Decode Chunk</a> algorithm with |chunk| and
        |output algorithm|.
  </dd>

  <dt><dfn method for=VideoDecoder>flush()</dfn></dt>
  <dd>
    Completes all <a>control messages</a> in the <a>control message queue</a>
    and emits all outputs.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>VideoFrame Output</a> algorithm.
    2. Run the <a>Flush</a> algorithm with |output algorithm|.
  </dd>

  <dt><dfn method for=VideoDecoder>reset()</dfn></dt>
  <dd>
    Immediately resets all state including configuration,
    <a>control messages</a> in the <a>control message queue</a>, and all pending
    callbacks.

    When invoked, run the <a>Reset</a> algorithm.
  </dd>

  <dt><df method for=VideoDecoder>close()</df></dt>
  <dd>
    Immediately aborts all pending work and releases system resources. Close is
    permanent.

    When invoked, run the <a>Close</a> algorithm.
  </dd>
</dl>


AudioEncoder Interface {#audioencoder-interface}
================================================

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface AudioEncoder {
  constructor(AudioEncoderInit init);
  readonly attribute CodecState state;
  readonly attribute long encodeQueueSize;
  void configure(AudioEncoderConfig config);
  void encode(AudioFrame frame);
  Promise<void> flush();
  void reset();
  void close();
};

dictionary AudioEncoderInit {
  required EncodedAudioChunkOutputCallback output;
  required WebCodecsErrorCallback error;
};

callback EncodedAudioChunkOutputCallback = void(EncodedAudioChunk output);
</xmp>
</pre>

Internal Slots {#audioencoder-internal-slots}
---------------------------------------------
<dl>
<dt><dfn for=AudioEncoder>[[codec implementation]]</dfn></dt>
<dd>Underlying encoder implementation provided by the User Agent.</dd>
<dt><dfn for=AudioEncoder>[[output callback]]</dfn></dt>
<dd>Callback given at construction for encoded outputs.</dd>
<dt><dfn for=AudioEncoder>[[error callback]]</dfn></dt>
<dd>Callback given at construction for encode errors.</dd>
</dl>

Constructors {#audioencoder-constructors}
-----------------------------------------
<dfn constructor for=AudioEncoder title="AudioEncoder(init)">
  AudioEncoder(init)
</dfn>
1. Let e be a new AudioEncoder object.
2. Assign init.output to the [[output callback]] internal slot.
3. Assign init.error to the [[error callback]] internal slot.
4. Assign "unconfigured" to e.state.
5. Return e.

Attributes {#audioencoder-attributes}
-------------------------------------
<dl>
  <dt>
    <dfn attribute for=AudioEncoder>state</dfn>
  </dt>
  <dd>Describes the current state of the codec.</dd>
  <dt>
    <dfn attribute for=AudioEncoder>encodeQueueSize</dfn>
  </dt>
  <dd>
    The number of pending encode requests. This does not include requests that
    have been sent to the underlying codec.
  </dd>
</dl>

Methods {#audioencoder-methods}
-------------------------------
<dl>
  <dt><dfn method for=AudioEncoder>configure(config)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to configure the audio encoder for
    decoding chunks as described by |config|.

    When invoked, run these steps:
    1. If |config| is not a <a>valid AudioEncoderConfig</a>, throw a
        {{TypeError}}.
    2. Run the <a>Configure Encoder</a> algorithm with |config|.
  </dd>

  <dt><dfn method for=AudioEncoder>encode(frame)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to encode the given |frame|.

    NOTE: This method will destroy the VideoFrame. Authors who wish to retain a
    copy, should call frame.clone() prior to calling encode().

    When invoked, run these steps:
    1. If the value of |frame|'s {{AudioFrame/[[detached]]}} internal slot is
        `true`, throw a {{TypeError}}.
    2. Let |output algorithm| be the <a>EncodedAudioChunk Output</a> algorithm.
    3. Run the <a>Encode Frame</a> algorithm with |frame| and
        |output algorithm|.
  </dd>

  <dt><dfn method for=AudioEncoder>flush()</dfn></dt>
  <dd>
    Completes all <a>control messages</a> in the <a>control message queue</a>
    and emits all outputs.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>EncodedAudioChunk Output</a> algorithm.
    2. Run the <a>Flush</a> algorithm with |output algorithm|.
  </dd>

  <dt><dfn method for=AudioEncoder>reset()</dfn></dt>
  <dd>
    Immediately resets all state including configuration,
    <a>control messages</a> in the <a>control message queue</a>, and all pending
    callbacks.

    When invoked, run the <a>Reset</a> algorithm.
  </dd>

  <dt><df method for=AudioEncoder>close()</df></dt>
  <dd>
    Immediately aborts all pending work and releases system resources. Close is
    permanent.

    When invoked, run the <a>Close</a> algorithm.
  </dd>
</dl>


VideoEncoder Interface {#videoencoder-interface}
================================================

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface VideoEncoder {
  constructor(VideoEncoderInit init);
  readonly attribute CodecState state;
  readonly attribute long encodeQueueSize;
  void configure(VideoEncoderConfig config);
  void encode(VideoFrame frame, optional VideoEncoderEncodeOptions options = {});
  Promise<void> flush();
  void reset();
  void close();
};

dictionary VideoEncoderInit {
  required EncodedVideoChunkOutputCallback output;
  required WebCodecsErrorCallback error;
};

callback EncodedVideoChunkOutputCallback = void(EncodedVideoChunk output);
</xmp>
</pre>

Internal Slots {#videoencoder-internal-slots}
---------------------------------------------
<dl>
<dt><dfn for=VideoEncoder>[[codec implementation]]</dfn></dt>
<dd>Underlying encoder implementation provided by the User Agent.</dd>
<dt><dfn for=VideoEncoder>[[output callback]]</dfn></dt>
<dd>Callback given at construction for encoded outputs.</dd>
<dt><dfn for=VideoEncoder>[[error callback]]</dfn></dt>
<dd>Callback given at construction for encode errors.</dd>
</dl>

Constructors {#videoencoder-constructors}
-----------------------------------------
<dfn constructor for=VideoEncoder title="VideoEncoder(init)">
  VideoEncoder(init)
</dfn>
1. Let e be a new VideoEncoder object.
2. Assign init.output to the [[output callback]] internal slot.
3. Assign init.error to the [[error callback]] internal slot.
4. Assign "unconfigured" to e.state.
5. Return e.

Attributes {#videoencoder-attributes}
-------------------------------------
<dl>
  <dt>
    <dfn attribute for=VideoEncoder>state</dfn>
  </dt>
  <dd>Describes the current state of the codec.</dd>
  <dt>
    <dfn attribute for=VideoEncoder>encodeQueueSize</dfn>
  </dt>
  <dd>
    The number of pending encode requests. This does not include requests that
    have been sent to the underlying codec.
  </dd>
</dl>

Methods {#videoencoder-methods}
-------------------------------
<dl>
  <dt><dfn method for=VideoEncoder>configure(config)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to configure the video encoder for
    decoding chunks as described by |config|.

    When invoked, run these steps:
    1. If |config| is not a <a>valid VideoEncoderConfig</a>, throw a
        {{TypeError}}.
    2. Run the <a>Configure Encoder</a> algorithm with |config|.
  </dd>

  <dt><dfn method for=VideoEncoder>encode(frame, options)</dfn></dt>
  <dd>
    <a>Enqueues a control message</a> to encode the given |frame|.

    NOTE: This method will destroy the VideoFrame. Authors who wish to retain a
    copy, should call frame.clone() prior to calling encode().

    When invoked, run these steps:
    1. If the value of |frame|'s {{VideoFrame/[[detached]]}} internal slot is
        `true`, throw a {{TypeError}}.
    2. Let |output algorithm| be the <a>EncodedVideoChunk Output</a> algorithm.
    3. Run the <a>Encode Frame</a> algorithm with |frame| and
        |output algorithm|.
  </dd>

  <dt><dfn method for=VideoEncoder>flush()</dfn></dt>
  <dd>
    Completes all <a>control messages</a> in the <a>control message queue</a>
    and emits all outputs.

    When invoked, run these steps:
    1. Let |output algorithm| be the <a>EncodedVideoChunk Output</a> algorithm.
    2. Run the <a>Flush</a> algorithm with |output algorithm|.
  </dd>

  <dt><dfn method for=VideoEncoder>reset()</dfn></dt>
  <dd>
    Immediately resets all state including configuration,
    <a>control messages</a> in the <a>control message queue</a>, and all pending
    callbacks.

    When invoked, run the <a>Reset</a> algorithm.
  </dd>

  <dt><df method for=VideoEncoder>close()</df></dt>
  <dd>
    Immediately aborts all pending work and releases system resources. Close is
    permanent.

    When invoked, run the <a>Close</a> algorithm.
  </dd>
</dl>


Decoder and Encoder Algorithms {#decoder-and-encoder-algorithms}
================================================================

The following algorithms run in the scope of the methods that invoke them.
Mentions of attributes and internal slots refer to members of the interface that
owns the invoking method.

<dfn>Configure Decoder</dfn>{#configure-decoder-algorithm}
----------------------------------------------------------
Given either an AudioDecoderConfig or VideoDecoderConfig |config|, this
algorithm attempts to select a codec implementation that supports |config|.

Run the following steps:
1. If `state` is `“closed”`, throw an {{InvalidStateError}}.
2. If the user agent cannot provide a codec implementation to support config,
    throw a {{NotSupportedError}}.
3. Set `state` to `"configured"`.
4. <a>Queue a control message</a> to configure the decoder with |config|.
5. <a>Run the control message processing loop</a>.

<a>Running a control message</a> to configure the decoder means running these
    steps:
1. Assign **[[codec implementation]]** with an implementation
    supporting |config|.


<dfn>Decode Chunk</dfn> (with |chunk| and |output algorithm|) {#decode-chunk-algorithm}
---------------------------------------------------------------------------------------
Run these steps:
1. If `state` is not `"configured"`, throw an {{InvalidStateError}}.
2. Increment `decodeQueueSize`.
3. <a>Queue a control message</a> to decode the |chunk|.
4. <a>Run the control message processing loop</a>.

Running a control message to decode the chunk means running these steps:
1. Decrement `decodeQueueSize`
2. Let |codec implementation queue| be the result of starting a new <a>parallel
    queue</a>.
3. Enqueue the following steps to |codec implementation queue|:
    1. Attempt to use **[[codec implementation]]** to decode the chunk.
    2. If decoding results in an error, queue a task on the media element task
        source to run the <a>Codec Error</a> algorithm.
4. Otherwise, for each output, queue a task on the media element task source to
    run the provided output algorithm.


<dfn>Flush</dfn>{#flush-algorithm}
----------------------------------
Given an |output algorithm|, this algorithm flushes all pending outputs to the
    output callback.

Run these steps:
1. If `state` is not `"configured"`, return a Promise rejected with a newly
    created {{InvalidStateError}}.
2. Let |promise| be a new Promise.
3. <a>Queue a control message</a> to flush the codec with |promise| and
    |output algorithm|
4. Return |promise|.

Running a control message to flush the codec means running these steps
    with |promise| and |output algorithm|.
1. Signal **[[codec implementation]]** to emit all pending outputs.
2. For each output, run |output algorithm|.
3. Resolve |promise|.

<dfn>Codec Error</dfn>{#codec-error-algorithm}
----------------------------------------------
This algorithm fires the error callback and permanently closes the codec.

Run these steps:
1. Cease processing of <a>control message queue</a>.
2. Run the <a>Close</a> algorithm with {{EncodingError}}.

<dfn>AudioFrame Output</dfn>{#audio-frame-output-algorithm}
-----------------------------------------------------------
Run these steps:
1. If `state` is not `“configured”`, abort the following steps.
2. Let |buffer| be an {{AudioBuffer}} containing the decoded audio data.
3. Let |frame| be an {{AudioFrame}} containing |buffer| and a timestamp for the
    output.
4. Invoke **[[output callback]]** with frame.

<dfn>VideoFrame Output</dfn>{#video-frame-output-algorithm}
-----------------------------------------------------------
Run these steps:
1. If state is not “configured”, abort the following steps.
2. Let |planes| be a sequence of {{Plane}}s containing the decoded video frame
    data.
3. Let |pixelFormat| be the {{PixelFormat}} of |planes|.
4. Let |frameInit| be a {{VideoFrameInit}} with the following keys:
    1. Let timestamp and duration be the presentation timestamp and duration
        from the EncodedVideoChunk associated with this output.
    2. Let codedWidth and codedHeight be the width and height of the decoded
        video frame in pixels, prior to any cropping or aspect ratio
        adjustments.
    3. Let cropLeft, cropTop, cropWidth, and cropHeight be the crop region of
        the decoded video frame in pixels, prior to any aspect ratio
        adjustments.
    4. Let displayWidth and displayHeight be the display size of the decoded
        video frame in pixels.
9. Let |frame| be a {{VideoFrame}}, constructed with |pixelFormat|, |planes|,
    and |frameInit|.
10. Invoke **[[output callback]]** with |frame|.

<dfn>Reset</dfn>{#reset-algorithm}
----------------------------------
Run these steps:
1. If `state` is `“closed”`, throw an {{InvalidStateError}}.
2. Set `state` to `“unconfigured”`.
3. Signal **[[codec implementation]]** to cease producing output
    for the previous configuration.

NOTE: Some tasks to emit outputs may already be queued in the event loop. These
    outputs will be dropped by the output algorithms, which abort if `state` is
    not `“configured”`.

4. For each <a>control message</a> in the <a>control message queue</a>:
    1. If a control message has an associated promise, reject the promise.
    2. Remove the message from the queue.

<dfn>Close</dfn> (with error){#close-algorithm}
-----------------------------------------------
Run these steps:
1. Run the <a>Reset</a> algorithm.
2. Set `state` to `“closed”`.
3. Clear **[[codec implementation]]** and release associated system
    resources.
4. If |error| is set, invoke **[[error callback]]** with |error|.

<dfn>Configure Encoder</dfn> (with config){#configure-encoder-algorithm}
------------------------------------------------------------------------
Run the following steps:
1. If `state` is `"closed"`, throw an {{InvalidStateError}}.
2. If the user agent cannot provide a codec implementation to support |config|,
    throw a {{NotSupportedError}}.
3. Set `state` to `"configured"`.
4. <a>Queue a control message</a> to configure the encoder using |config|.
5. <a>Run the control message processing loop</a>.

Running a control message to configure the encoder means running these steps:
1. Assign **[[codec implementation]]** with an implementation
    supporting |config|.

<dfn>Encode Frame</dfn> (with frame, options, and output algorithm){#encode-frame-algorithm}
--------------------------------------------------------------------------------------------
Run these steps:
1. If `state` is not `"configured"`, throw an {{InvalidStateError}}.
2. If the value of |frame|'s **\[[detached]]** internal slot is
    `true`, throw a {{TypeError}}.
3. Let |frameClone| hold the result of running the <a>Clone Frame</a> algorithm
    with |frame|.
4. Destroy the original |frame| by invoking frame.destroy().
5. Increment `encodeQueueSize`.
6. <a>Queue a control message</a> to encode |frameClone| with |options| and
    |output algorithm|.
7. Run the control message processing loop.

Running a control message to encode the frame means running these steps.
1. Decrement `encodeQueueSize`.
2. Let |codec implementation queue| be the result of starting a new
    <a>parallel queue</a>.
3. Enqueue the following steps to |codec implementation queue|:
    1. Attempt to use **[[codec implementation]]** and options to encode
        |frameClone|.
    2. If encoding results in an error, queue a task on the media element task
        source to run the codec error algorithm.
    3. Otherwise, for each output, queue a task on the media element task source
        to run the provided output algorithm.

<dfn>EncodedAudioChunk Output</dfn>{#encodedaudiochunk-output-algorithm}
------------------------------------------------------------------------
Run these steps:
1. If `state` is not `“configured”`, abort the following steps.
2. Let |chunkInit| be an {{EncodedAudioChunkInit}} with the following keys:
    1. Let data contain the encoded audio data.
    2. Let type be the EnocdedAudioChunkType of the encoded audio data.
    3. Let timestamp be the timestamp from the associated input AudioFrame.
    4. Let duration be the duration from the associated input AudioFrame.
7. Let |chunk| be a new {{EncodedAudioChunk}} constructed with |chunkInit|.
8. Invoke [[output callback]] with |chunk|.

<dfn>EncodedVideoChunk Output</dfn>{#encodedvideochunk-output-algorithm}
------------------------------------------------------------------------
Run these steps:
1. If `state` is not `“configured”`, abort the following steps.
2. Let |chunkInit| be an {{EncodedVideoChunkInit}} with the following keys:
    1. Let data contain the encoded video data.
    2. Let type be the {{EncodedVideoChunkType}} of the encoded video data.
    3. Let timestamp be the timestamp from the associated input {{VideoFrame}}.
    4. Let duration be the duration from the associated input {{VideoFrame}}.
3. Let |chunk| be a new {{EncodedVideoChunk}} constructed with |chunkInit|.
4. Invoke [[output callback]] with chunk.

Configurations{#configurations}
===============================

<dfn>Codec String</dfn>{#config-codec-string}
--------------------------------------------
In other media specifications, codec strings historically accompanied
    [[mime types]] as the “codecs=” parameter
    ({{MediaSource/isTypeSupported()}}, {{HTMLMediaElement/canPlayType()}}).
    In this specification, encoded media is not containerized; hence, only the
    value of the codecs parameter is accepted.

A <dfn>valid codec string</dfn> must meet the following conditions.
1. Is valid per the relevant codec specification (see examples below).

NOTE: This needs more work. We might consider a registry of specs/strings.

2. It describes a single codec.

NOTE: Not a comma separated list.

3. It is unambiguous about codec profile and level for codecs that define these
    concepts.


NOTE: There is no unified specification for codec strings. Each codec has its
    own unique string format, specified by the authors of the codec. Relevant
    specifications include:
        * h264, aac - [[RFC6381]]
        * vp9 - https://www.webmproject.org/vp9/mp4/#codecs-parameter-string,
        * hevc - ISO IEC 14496-15 dated 2012 or newer in the Annex E.3
        * av1 - https://aomediacodec.github.io/av1-isobmff/#codecsparam,

NOTE: Some valid examples include:
    'vp8', 'vp09.00.10.08', 'avc1.4D401E', 'opus', 'mp4a.40.2', 'flac'

    Invalid examples include:
    'video/webm; codecs="vp8"' (invalid to supply full mimetype; valid as just 'vp8')
    'codecs="opus"' (invalid to include codecs= prefix)
    ‘flac,vorbis’ (describes more than one codec)
    ‘vp9’ (ambiguous about profile and level)
    'video/mp4' (describes a container, not a codec)


<dfn>AudioDecoderConfig</dfn>{#audio-decoder-config}
----------------------------------------------------
<pre class='idl'>
<xmp>
dictionary AudioDecoderConfig {
  required DOMString codec;
  required unsigned long sampleRate;
  required unsigned long numberOfChannels;
  BufferSource description;
};
</xmp>
</pre>

To check if an <a>AudioDecoderConfig</a> is a <dfn>valid
    AudioDecoderConfig</dfn>, run these steps:
1. If codec is not a <a>valid codec string</a>, return `false`.
2. Return `true`.

<dl>
  <dt><dfn dict-member for=AudioDecoderConfig>codec</dfn></dt>
  <dd>Contains a codec string describing the codec.</dd>

  <dt><dfn dict-member for=AudioDecoderConfig>sampleRate</dfn></dt>
  <dd>The number of frame samples per second.</dd>

  <dt><dfn dict-member for=AudioDecoderConfig>numberOfChannels</dfn></dt>
  <dd>The number of audio channels.</dd>

  <dt><dfn dict-member for=AudioDecoderConfig>description</dfn></dt>
  <dd>
    A sequence of codec specific bytes, commonly known as extradata.

    NOTE: For example, the vorbis “code book”.
  </dd>
</dl>


<dfn>VideoDecoderConfig</dfn>{#video-decoder-config}
----------------------------------------------------
<pre class='idl'>
<xmp>
dictionary VideoDecoderConfig {
  required DOMString codec;
  BufferSource description;
  required unsigned long codedWidth;
  required unsigned long codedHeight;
  unsigned long cropLeft;
  unsigned long cropTop;
  unsigned long cropWidth;
  unsigned long cropHeight;
  unsigned long displayWidth;
  unsigned long displayHeight;
};
</xmp>
</pre>

To check if a {{VideoDecoderConfig}} is a <dfn>valid VideoDecoderConfig</dfn>,
run these steps:
1. If {{VideoDecoderConfig/codec}} is not a <a>valid codec string</a>, return `false`.
2. If {{VideoDecoderConfig/codedWidth}} = 0 or {{VideoDecoderConfig/codedHeight}} = 0, return `false`.
3. If {{VideoDecoderConfig/cropWidth}} = 0 or {{VideoDecoderConfig/cropHeight}} = 0, return `false`.
4. If {{VideoDecoderConfig/cropTop}} + {{VideoDecoderConfig/cropHeight}} >= {{VideoDecoderConfig/codedHeight}}, return `false`.
5. If {{VideoDecoderConfig/cropLeft}} + {{VideoDecoderConfig/cropWidth}} >= {{VideoDecoderConfig/codedWidth}}, return `false`.
6. If {{VideoDecoderConfig/displayWidth}} = 0 or {{VideoDecoderConfig/displayHeight}} = 0, return `false`.
7. Return `true`.

<dl>
  <dt><dfn dict-member for=VideoDecoderConfig>codec</dfn></dt>
  <dd>Contains a codec string describing the codec.</dd>

  <dt><dfn dict-member for=VideoDecoderConfig>description</dfn></dt>
  <dd>
    A sequence of codec specific bytes, commonly known as extradata.

    NOTE: For example, the VP9 vpcC bytes.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>codedWidth</dfn></dt>
  <dd>
    Width of the VideoFrame in pixels, prior to any cropping or aspect ratio
        adjustments.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>codedHeight</dfn></dt>
  <dd>
    Height of the VideoFrame in pixels, prior to any cropping or aspect ratio
        adjustments.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>cropLeft</dfn></dt>
  <dd>
    The number of pixels to remove from the left of the VideoFrame, prior to
        aspect ratio adjustments. Defaults to zero if not present.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>cropTop</dfn></dt>
  <dd>
    The number of pixels to remove from the top of the VideoFrame, prior to
        aspect ratio adjustments. Defaults to zero if not present.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>cropWidth</dfn></dt>
  <dd>
    The width of pixels to include in the crop, starting from cropLeft.
        Defaults to codedWidth if not present.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>cropHeight</dfn></dt>
  <dd>
    The height of pixels to include in the crop, starting from cropLeft.
        Defaults to codedHeight if not present.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>displayWidth</dfn></dt>
  <dd>
    Width of the VideoFrame when displayed. Defaults to cropWidth if not
        present.
  </dd>

  <dt><dfn dict-member for=VideoDecoderConfig>displayHeight</dfn></dt>
  <dd>
    Height of the VideoFrame when displayed. Defaults to cropHeight if not
        present.
  </dd>
</dl>


<dfn>AudioEncoderConfig</dfn>{#audio-encoder-config}
----------------------------------------------------
<pre class='idl'>
<xmp>
dictionary AudioEncoderConfig {
  required DOMString codec;
  unsigned long sampleRate;
  unsigned long numberOfChannels;
};
</xmp>
</pre>

To check if an {{AudioEncoderConfig}} is a <dfn>valid AudioEncoderConfig</dfn>,
run these steps:
1. If {{AudioEncoderConfig/codec}} is not a <a>valid codec string</a>, return `false`.
2. Return `true`.

<dl>
  <dt><dfn dict-member for=AudioEncoderConfig>codec</dfn></dt>
  <dd>Contains a codec string describing the codec.</dd>

  <dt><dfn dict-member for=AudioEncoderConfig>sampleRate</dfn></dt>
  <dd>The number of frame samples per second.</dd>

  <dt><dfn dict-member for=AudioEncoderConfig>numberOfChannels</dfn></dt>
  <dd>The number of audio channels.</dd>
</dl>


<dfn>VideoEncoderConfig</dfn>{#video-encoder-config}
----------------------------------------------------
<pre class='idl'>
<xmp>
dictionary VideoEncoderConfig {
  required DOMString codec;
  unsigned long long bitrate;
  required unsigned long width;
  required unsigned long height;
};
</xmp>
</pre>

To check if a {{VideoEncoderConfig}} is a <dfn>valid VideoEncoderConfig</dfn>, run
these steps:
1. If {{VideoEncoderConfig/codec}} is not a <a>valid codec string</a>, return `false`.
2. If {{VideoEncoderConfig/width}} = 0 or {{VideoEncoderConfig/height}} = 0, return `false`.
4. Return `true`.

<dl>
  <dt><dfn dict-member for=VideoEncoderConfig>width</dfn></dt>
  <dd>The expected cropWidth of input VideoFrames to encode.</dd>

  <dt><dfn dict-member for=VideoEncoderConfig>height</dfn></dt>
  <dd>The expected cropHeight of input VideoFrames to encode.</dd>
</dl>


<dfn>VideoEncoderEncodeOptions</dfn>{#video-encoder-options}
------------------------------------------------------------

<pre class='idl'>
<xmp>
dictionary VideoEncoderEncodeOptions {
  boolean keyFrame;
};
</xmp>
</pre>

<dl>
  <dt><dfn dict-member for=VideoEncoderEncodeOptions>keyFrame</dfn></dt>
  <dd>Indicates whether the given frame MUST be encoded as a key frame.</dd>
</dl>


<dfn>CodecState</dfn>{#codec-state}
-----------------------------------
<pre class='idl'>
<xmp>
enum CodecState {
  "unconfigured",
  "configured",
  "closed"
};
</xmp>
</pre>

<dl>
  <dt><dfn enum-value for=CodecState>unconfigured</dfn></dt>
  <dd>The codec is not configured for encoding or decoding.</dd>
  <dt><dfn enum-value for=CodecState>configured</dfn></dt>
  <dd>
    A valid configuration has been provided. The codec is ready for encoding or
        decoding.
  </dd>
  <dt><dfn enum-value for=CodecState>closed</dfn></dt>
  <dd>
    The codec is no longer usable and underlying system resources have been
      released.
  </dd>
</dl>

<dfn>WebCodecsErrorCallback</dfn>{#error-callback}
--------------------------------------------------
<pre class='idl'>
<xmp>
callback WebCodecsErrorCallback = void(DOMException error);
</xmp>
</pre>


Encoded Media Interfaces (Chunks) {#encoded-media-interfaces}
=============================================================
These interfaces represent chunks of encoded media.

EncodedAudioChunk Interface {#encodedaudiochunk-interface}
------------------------------------------------------------
<pre class='idl'>
<xmp>
interface EncodedAudioChunk {
  constructor(EncodedAudioChunkInit init);
  readonly attribute EncodedAudioChunkType type;
  readonly attribute unsigned long long timestamp;  // microseconds
  readonly attribute ArrayBuffer data;
};

dictionary EncodedAudioChunkInit {
  required EncodedAudioChunkType type;
  required unsigned long long timestamp;
  required BufferSource data;
};

enum EncodedAudioChunkType {
    "key",
    "delta",
};
</xmp>
</pre>

### Constructors ### {#encodedaudiochunk-constructors}
<dfn constructor for=EncodedAudioChunk title="EncodedAudioChunk(init)">
  EncodedAudioChunk(init)
</dfn>
1. Let |chunk| be a new {{EncodedAudioChunk}} object, initialized as follows
    1. Assign init.type to chunk.type.
    2. Assign init.timestamp to chunk.timestamp.
    3. Assign a copy of init.data to chunk.data.
5. Return |chunk|.

### Attributes ### {#encodedaudiochunk-attributes}
<dl>
  <dt><dfn attribute for=EncodedAudioChunk>type</dfn></dt>
  <dd>Describes whether the chunk is a key frame.</dd>

  <dt><dfn attribute for=EncodedAudioChunk>timestamp</dfn></dt>
  <dd>The presentation timestamp, given in microseconds.</dd>

  <dt><dfn attribute for=EncodedAudioChunk>data</dfn></dt>
  <dd>A sequence of bytes containing encoded audio data.</dd>
</dl>

EncodedVideoChunk Interface{#encodedvideochunk-interface}
-----------------------------------------------------------
<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface EncodedVideoChunk {
  constructor(EncodedVideoChunkInit init);
  readonly attribute EncodedVideoChunkType type;
  readonly attribute unsigned long long timestamp;  // microseconds
  readonly attribute unsigned long long? duration;  // microseconds
  readonly attribute ArrayBuffer data;
};

dictionary EncodedVideoChunkInit {
  required EncodedVideoChunkType type;
  required unsigned long long timestamp;
  unsigned long long duration;
  required BufferSource data;
};

enum EncodedVideoChunkType {
    "key",
    "delta",
};
</xmp>
</pre>

### Constructors ### {#encodedvideochunk-constructors}
<dfn constructor for=EncodedVideoChunk title="EncodedVideoChunk(init)">
  EncodedVideoChunk(init)
</dfn>
1. Let |chunk| be a new {{EncodedVideoChunk}} object, initialized as follows
    1. Assign init.type to chunk type.
    2. Assign init.timestamp to chunk.timestamp.
    3. If duration is present in init, assign init.duration to chunk.duration.
        Otherwise, assign null to chunk.duration.
2. Assign a copy of init.data to chunk.data.
3. Return |chunk|.

### Attributes ### {#encodedvideochunk-attributes}
<dl>
  <dt><dfn attribute for=EncodedVideoChunk>type</dfn></dt>
  <dd>Describes whether the chunk is a key frame or not.</dd>

  <dt><dfn attribute for=EncodedVideoChunk>timestamp</dfn></dt>
  <dd>The presentation timestamp, given in microseconds.</dd>

  <dt><dfn attribute for=EncodedVideoChunk>duration</dfn></dt>
  <dd>The presentation duration, given in microseconds.</dd>

  <dt><dfn attribute for=EncodedVideoChunk>data</dfn></dt>
  <dd>A sequence of bytes containing encoded video data.</dd>
</dl>


Raw Media Interfaces (Frames){#raw-media-interfaces}
====================================================
These interfaces represent unencoded (raw) media.


AudioFrame Interface {#audioframe-interface}
---------------------------------------------

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface AudioFrame {
  constructor(AudioFrameInit init);
  readonly attribute unsigned long long timestamp;
  readonly attribute AudioBuffer? buffer;
  void close();
};

dictionary AudioFrameInit {
  required unsigned long long timestamp;
  required AudioBuffer buffer;
};
</xmp>
</pre>

### Internal Slots ###{#audioframe-internal-slots}
<dl>
  <dt><dfn attribute for=AudioFrame>\[[detached]]</dfn></dt>
  <dd>
    Boolean indicating whether close() was invoked and underlying resources
        have been released.
  </dd>
</dl>


### Constructors ###{#audioframe-constructors}
<dfn constructor for=AudioFrame title="AudioFrame(init)">
  AudioFrame(init)
</dfn>
1. Let |frame| be a new {{AudioFrame}} object.
2. Assign init.timestamp to frame.timestamp.
3. Assign init.buffer to frame.buffer.
<!-- 4. Assign `false` to the {{AudioFrame/[[foo]]}} internal slot. -->
5. Return |frame|.


### Attributes ###{#audioframe-attributes}
<dl>
  <dt><dfn attribute for=AudioFrame>timestamp</dfn></dt>
  <dd>The presentation timestamp, given in microseconds.</dd>

  <dt><dfn attribute for=AudioFrame>buffer</dfn></dt>
  <dd>The buffer containing decoded audio data.</dd>
</dl>


### Methods ###{#audioframe-methods}
<dl>
  <dt><dfn method for=AudioFrame>close()</dfn></dt>
  <dd>
    Immediately frees system resources. When invoked, run these steps:
    1. Release system resources for buffer and set its value to null.
    2. Assign `true` to the {{AudioFrame/[[detached]]}} internal slot.

    NOTE: This section needs work. We should use the name and semantics of
        VideoFrame destroy(). Similarly, we should add clone() to make a deep
        copy.
  </dd>
</dl>

VideoFrame Interface {#videoframe-interface}
--------------------------------------------

<pre class='idl'>
<xmp>
[Exposed=(Window,Worker)]
interface VideoFrame {
  constructor(ImageBitmap imageBitmap, VideoFrameInit frameInit);
  constructor(PixelFormat pixelFormat, sequence<(Plane or PlaneInit)> planes,
              VideoFrameInit frameInit);

  readonly attribute PixelFormat format;
  readonly attribute FrozenArray<Plane> planes;
  readonly attribute unsigned long codedWidth;
  readonly attribute unsigned long codedHeight;
  readonly attribute unsigned long cropLeft;
  readonly attribute unsigned long cropTop;
  readonly attribute unsigned long cropWidth;
  readonly attribute unsigned long cropHeight;
  readonly attribute unsigned long displayWidth;
  readonly attribute unsigned long displayHeight;
  readonly attribute unsigned long long? duration;
  readonly attribute unsigned long long? timestamp;

  void destroy();
  VideoFrame clone();

  Promise<ImageBitmap> createImageBitmap(
    optional ImageBitmapOptions options = {});

};

dictionary VideoFrameInit {
  unsigned long codedWidth;
  unsigned long codedHeight;
  unsigned long cropLeft;
  unsigned long cropTop;
  unsigned long cropWidth;
  unsigned long cropHeight;
  unsigned long displayWidth;
  unsigned long displayHeight;
  unsigned long long duration;
  unsigned long long timestamp;
};
</xmp>
</pre>

### Internal Slots ###{#videoframe-internal-slots}
<dl>
  <dt><dfn attribute for=VideoFrame>\[[detached]]</dfn></dt>
  <dd>
    Boolean indicating whether {{destroy()}} was invoked and underlying
        resources have been released.
  </dd>
</dl>

### Constructors ###{#videoframe-constructors}

NOTE: this section needs work. Current wording assumes a VideoFrame can always
    be easily represented using one of the known pixel formats. In practice, the
    underlying UA resources may be GPU backed or formatted in such a way that
    conversion to an allowed pixel format requires expensive copies and
    translation. When this occurs, we should allow planes to be null and format
    to be “opaque” to avoid early optimization. We should make conversion
    explicit and user controlled by offering a videoFrame.convertTo(format) that
    returns a Promise containing a new VideoFrame for which the
    copies/translations are performed.

<dfn constructor for=VideoFrame title="VideoFrame(imageBitmap, frameInit)">
  VideoFrame(imageBitmap, frameInit)
</dfn>
1. If |frameInit| is not a <a>valid VideoFrameInit</a>, throw a {{TypeError}}.
2. If the value of |imageBitmap|'s' {{ImageBitmap/[[Detached]]}} internal slot
    is set to `true`, then throw an {{InvalidStateError}} DOMException.
3. Let |frame| be a new {{VideoFrame}}.
4. Assign `false` to |frame|’s {{VideoFrame/[[detached]]}} internal slot.
5. Use a copy of the pixel data in |imageBitmap| to initialize to following
    frame attributes:
    1. Initialize frame.pixelFormat be the underlying format of imageBitmap.
    2. Initialize frame.planes to describe the arrangement of memory of the
        copied pixel data.
    3. Assign regions of the copied pixel data to the
        {{Plane/[[plane buffer]]}} internal slot of each plane as
        appropriate for the pixel format.
    4. Initialize frame.codedWidth and frame.codedHeight describe the width and
        height of the imageBitamp prior to any cropping or aspect ratio
        adjustments.
6. Use |frameInit| to initialize the remaining frame attributes:
    1. If frameInit.cropLeft is present, initialize it frame.cropLeft.
        Otherwise, default frame.cropLeft to zero.
    2. If frameInit.cropTop is present, initialize it to frame.cropTop.
        Otherwise, default frame.cropTop to zero.
    3. If frameInit.cropWidth is present, initialize it to frame.cropWidth.
        Otherwise, default frame.cropWidth to frame.codedWidth.
    4. If frameInit.cropHeight is present, initialize it to frame.cropHeight.
        Otherwise, default frame.cropHeight to frame.codedHeight.
    5. If frameInit.displayWidth is present, initialize it to
        frame.displayWidth. Otherwise, default frame.displayWidth to
        frame.codedWidth.
    6. If frameInit.displayHeight is present, initialize it to
        frame.displayHeight. Otherwise, default frame.displayHeight to
        frame.codedHeight.
    7. If frameInit.duration is present, initialize it to frame.duration.
        Otherwise, default frame.duration to null.
    8. If frameInit.timestamp is present, initialize it to frame.timestamp.
        Otherwise default frame.timestamp to null.
7. Return |frame|.

<dfn constructor for=VideoFrame title="VideoFrame(pixelFormat, planes, frameInit)">
  VideoFrame(pixelFormat, planes, frameInit)
</dfn>
1. If either {{VideoFrameInit/codedWidth}} or {{VideoFrameInit/codedHeight}} is
    not present in |frameInit|, throw a {{TypeError}}.
2. If |frameInit| is not a <a>valid VideoFrameInit</a>, throw a {{TypeError}}.
3. If the length of |planes| is incompatible with the given pixelFormat, throw a
    TypeError.
4. Let |frame| be a new {{VideoFrame}} object.
5. Assign `false` to |frame|’s {{VideoFrame/[[detached]]}} internal slot.
6. Assign init.pixelFormat to frame.pixelFormat.
7. For each element |p| in |planes|:
    1. If |p| is a {{Plane}}, append a copy of p to frame.planes. Continue
        processing the next element.
    2. If |p| is a {{PlaneInit}}, append a new {{Plane}} <var ignore>q</var> to frame.planes
        initialized as follows:
        2. Assign a copy of p.src to q's [[plane buffer]] internal slot.

        NOTE: the samples should be copied exactly, but the user agent may add
            row padding as needed to improve memory alignment.

        3. Assign the width of each row in [[plane buffer]], including any
            padding, to  q.stride.
        4. Assign p.rows to q.rows.
        5. Assign the product of (q.rows * q.stride) to q.length
8. Assign frameInit.codedWidth to frame.codedWidth.
9. Assign frameInit.codedHeight to frame.codedHeight.
10. If frameInit.cropLeft is present, assign it frame.cropLeft. Otherwise,
    default frame.cropLeft to zero.
11. If frameInit.cropTop is present, assign it to frame.cropTop. Otherwise,
    default frame.cropTop to zero.
12. If frameInit.cropWidth is present, assign it to frame.cropWidth. Otherwise,
    default frame.cropWidth to frame.codedWidth.
13. If frameInit.cropHeight is present, assign it to frame.cropHeight.
    Otherwise, default frame.cropHeight to frame.codedHeight.
14. If frameInit.displayWidth is present, assign it to frame.displayWidth.
    Otherwise, default frame.displayWidth to frame.codedWidth.
15. If frameInit.displayHeight is present, assign it to frame.displayHeight.
    Otherwise, default frame.displayHeight to frame.codedHeight.
16. If frameInit.duration is present, assign it to frame.duration. Otherwise,
    default frame.duration to null.
17. If frameInit.timestamp is present, assign it to frame.timestamp. Otherwise
    default frame.timestamp to null.
18. Return frame.

### Attributes ###{#videoframe-attributes}
<dl>
  <dt><dfn attribute for=VideoFrame>timestamp</dfn></dt>
  <dd>
    The presentation timestamp, given in microseconds. The timestamp is copied
        from the EncodedVideoChunk corresponding to this VideoFrame.
  </dd>
  <dt><dfn attribute for=VideoFrame>duration</dfn></dt>
  <dd>
    The presentation duration, given in microseconds. The duration is copied from
        the EncodedVideoChunk corresponding to this VideoFrame.
  </dd>
  <dt><dfn attribute for=VideoFrame>format</dfn></dt>
  <dd>
    Describes the arrangement of bytes in each plane as well as the number and
        order of the planes.
  </dd>
  <dt><dfn attribute for=VideoFrame>planes</dfn></dt>
  <dd>
    Holds pixel data data, laid out as described by format and Plane
        attributes.
  </dd>
  <dt><dfn attribute for=VideoFrame>codedWidth</dfn></dt>
  <dd>
    Width of the VideoFrame in pixels, prior to any cropping or aspect ratio
        adjustments.
  </dd>
  <dt><dfn attribute for=VideoFrame>codedHeight</dfn></dt>
  <dd>
    Height of the VideoFrame in pixels, prior to any cropping or aspect ratio
        adjustments.
  </dd>
  <dt><dfn attribute for=VideoFrame>cropLeft</dfn></dt>
  <dd>
    The number of pixels to remove from the left of the VideoFrame, prior to
        aspect ratio adjustments.
  </dd>
  <dt><dfn attribute for=VideoFrame>cropTop</dfn></dt>
  <dd>
    The number of pixels to remove from the top of the VideoFrame, prior to
        aspect ratio adjustments.
  </dd>
  <dt><dfn attribute for=VideoFrame>cropWidth</dfn></dt>
  <dd>The width of pixels to include in the crop, starting from cropLeft.</dd>
  <dt><dfn attribute for=VideoFrame>cropHeight</dfn></dt>
  <dd>The height of pixels to include in the crop, starting from cropLeft.</dd>
  <dt><dfn attribute for=VideoFrame>displayWidth</dfn>
  <dd>Width of the VideoFrame when displayed.</dd>
  <dt><dfn attribute for=VideoFrame>displayHeight</dfn></dt>
  <dd>Height of the VideoFrame when displayed.</dd>
</dl>

### Methods ###{#videoframe-methods}
<dfn method for=VideoFrame>destroy()</dfn>
Immediately frees system resources. Destruction applies to all references,
    including references that are serialized and passed across Realms.

NOTE: Use clone() to create a deep copy. Cloned frames have their own lifetime
    and will not be affected by destroying the original frame.

When invoked, run these steps:
1. If {{VideoFrame/[[detached]]}} is `true`, throw an {{InvalidStateError}}.
2. Remove all {{Plane}}s from {{VideoFrame/planes}} and release associated
    memory.
3. Assign `true` to the {{VideoFrame/[[detached]]}} internal slot.

<dfn method for=VideoFrame>clone()</dfn>
Creates a new {{VideoFrame}} with a separate lifetime containing a deep copy of
    this frame’s resources.

NOTE:  VideoFrames may require a large amount of memory. Use
    {{VideoFrame/clone()}} sparingly. Authors should take care to manage frame
    lifetimes by calling {{VideoFrame/destroy()}} immediately when frames are no
    longer needed.

When invoked, run the following steps:
1. If the value of the {{VideoFrame/[[detached]]}} slot is `true`, return a
    Promise rejected with a newly created {{InvalidStateError}}.
2. Let |p| be a new Promise.
3. In parallel, resolve |p| with the result of running the <a>Clone Frame</a>
    algorithm with <a>this</a>.
4. Return |p|.

<dfn method for=VideoFrame>createImageBitmap(options)</dfn>
Creates an ImageBitmap from this {{VideoFrame}}.

When invoked, run these steps:
1. Let |p| be a new Promise.
2. If either |options|'s {{ImageBitmapOptions/resizeWidth}} or
    {{ImageBitmap/resizeHeight}} is present and is 0, then return |p| rejected
    with an {{InvalidStateError}} {{DOMException}}.
3. If the <a>this'</a> {{VideoFrame/[[detached]]}} internal slot is set to `true`,
    then return |p| rejected with an {{InvalidStateError}} {{DOMException}}.
4. Let |imageBitmap| be a new {{ImageBitmap}} object.
5. Set |imageBitmap|'s bitmap data to a copy of the {{VideoFrame}} pixel data,
    at the frame's intrinsic width and intrinsic height (i.e., after any
    aspect-ratio correction has been applied), cropped to the source rectangle
    with formatting.
6. If the origin of |imageBitmap|'s image is not same origin with entry settings
    object's origin, then set the origin-clean flag of |imageBitmap|'s bitmap to
    `false`.
7. Run this step in parallel:
  1. Resolve p with imageBitmap.

### Algorithms ###{#videoframe-algorithms}
To check if a {{VideoDecoderConfig}} is a <dfn>valid VideoFrameInit</dfn>,
run these steps:
1. If {{VideoFrameInit/codedWidth}} = 0 or {{VideoFrameInit/codedHeight}} = 0, return `false`.
2. If {{VideoFrameInit/cropWidth}} = 0 or {{VideoFrameInit/cropHeight}} = 0, return `false`.
3. If {{VideoFrameInit/cropTop}} + {{VideoFrameInit/cropHeight}} >= {{VideoFrameInit/codedHeight}}, return `false`.
4. If {{VideoFrameInit/cropLeft}} + {{VideoFrameInit/cropWidth}} >= {{VideoFrameInit/codedWidth}}, return `false`.
5. If {{VideoFrameInit/displayWidth}} = 0 or {{VideoFrameInit/displayHeight}} = 0, return `false`.
6. Return `true`.


Plane Interface {#plane-interface}
----------------------------------
A {{Plane}} acts like a thin wrapper around an {{ArrayBuffer}}, but may actually
    be backed by a texture. {{Plane}}s hide any padding before the first sample
    or after the last row.

A {{Plane}} is solely constructed by its {{VideoFrame}}. During construction,
    the User Agent may use knowledge of the frame’s {{PixelFormat}} to add
    padding to the {{Plane}} to improve memory alignment.

A {{Plane}} cannot be used after the {{VideoFrame}} is destroyed. A new
    {{VideoFrame}} can be assembled from existing {{Plane}}s, and the new
    {{VideoFrame}} will remain valid when the original is destroyed. This makes
    it possible to efficiently add an alpha plane to an existing {{VideoFrame}}.


<pre class='idl'>
<xmp>
interface Plane {
  readonly attribute unsigned long stride;
  readonly attribute unsigned long rows;
  readonly attribute unsigned long length;

  void readInto(ArrayBufferView dst);
};

dictionary PlaneInit {
  required BufferSource src;
  required unsigned long stride;
  required unsigned long rows;
};
</xmp>
</pre>

### Internal Slots ###{#plane-internal-slots}
<dl>
  <dt><dfn attribute for=Plane>[[parent frame]]</dfn></dt>
  <dd>Refers to the {{VideoFrame}} that constructed and owns this plane.</dd>
  <dt><dfn attribute for=Plane>[[plane buffer]]</dfn></dt>
  <dd>Internal storage for the plane’s pixel data.</dd>
</dl>

### Attributes ###{#plane-attributes}
<dl>
  <dt><dfn attribute for=Plane>stride</dfn></dt>
  <dd>The width of each row including any padding.</dd>
  <dt><dfn attribute for=Plane>rows</dfn></dt>
  <dd>The number of rows.</dd>
  <dt><dfn attribute for=Plane>length</dfn></dt>
  <dd>The total byte length of the plane (stride * rows).</dd>
</dl>

### Methods ###{#plane-methods}
<dfn method for=Plane>readInto(dst)</dfn>

Copies the plane data into dst.

When invoked, run these steps:
1. If {{Plane/[[parent frame]]}} has been destroyed, throw an
    {{InvalidStateError}}.
2. If {{Plane/length}} is greater than |dst.byteLength|, throw a {{TypeError}}.
3. Copy the {{Plane/[[plane buffer]]}} into <var ignore>dst</var>.


Pixel Format{#pixel-format}
---------------------------
Pixel formats describe the arrangement of bytes in each plane as well as the
number and order of the planes.

NOTE: This section needs work. We expect to add more pixel formats and offer
    much more verbose definitions. For now, please see
    <a href="http://www.fourcc.org/pixel-format/yuv-i420/">
    http://www.fourcc.org/pixel-format/yuv-i420/</a> for a more complete
    description.

<pre class='idl'>
<xmp>
enum PixelFormat {
  "I420"
};
</xmp>
</pre>

<dl>
  <dt><dfn enum-value for=PixelFormat>I420</dfn></dt>
  <dd>
    Planar 4:2:0 YUV.
  </dd>
</dl>


Algorithms{#raw-media-algorithms}
---------------------------------

<dfn>Clone Frame</dfn>{#clone-frame} (with |frame|)
1. Let cloneFrame be a new object of the same type as frame (either
    {{AudioFrame}} or {{VideoFrame}}).
2. Initialize each attribute and internal slot of clone with a copy of the value
    from the corresponding attribute of this frame.

NOTE: User Agents are encouraged to avoid expensive copies of large objects (for
    instance, {{VideoFrame}} pixel data). Frame types are immutable, so the
    above step may be implemented using memory sharing techniques such as
    reference counting.

3. Return cloneFrame.


VideoTrackReader Interface{#videotrackreader-interface}
=======================================================
{{VideoTrackReader}} emits {{VideoFrame}}s from a {{MediaStreamTrack}}. Authors
may use this interface to manipulate, render, or encode streams from
{{getUserMedia()}} and {{getDisplayMedia()}}.

<pre class='idl'>
<xmp>
[Exposed=Window]
interface VideoTrackReader {
  constructor(MediaStreamTrack track);

  readonly attribute VideoTrackReaderState readyState;
  attribute EventHandler onended;

  void start(VideoFrameOutputCallback callback);
  void stop();
};

enum VideoTrackReaderState {
  "started",
  "stopped",
  "ended"
};
</xmp>
</pre>

VideoTrackReaderState Values{#videotrackreaderreadystate}
--------------------------------------------------------------
<dl>
  <dt><dfn enum-value for=VideoTrackReaderState>started</dfn></dt>
  <dd>
    Indicates that the {{VideoTrackReader/[[track]]}} is
        {{MediaStreamTrackState/live}} and {{VideoFrame}}s are being output to
        the {{VideoTrackReader/[[callback]]}} provided to
        {{VideoTrackReader/start()}}.

  </dd>
  <dt><dfn enum-value for=VideoTrackReaderState>stopped</dfn></dt>
  <dd>
    Indicates that the {{VideoTrackReader/[[track]]}} is
        {{MediaStreamTrackState/live}}, but the
        {{VideoTrackReader/[[callback]]}} is not set, so no {{VideoFrame}}s are
        being output.
  </dd>
  <dt><dfn enum-value for=VideoTrackReaderState>ended</dfn></dt>
  <dd>
    Indicates that the {{VideoTrackReader/[[track]]}} is
        {{MediaStreamTrackState/ended}} and this object can no longer be
        {{VideoTrackReaderState/started}} nor {{VideoTrackReaderState/stopped}}.
  </dd>
</dl>

Internal Slots {#videotrackreader-slots}
----------------------------------------
<dl>
  <dt><dfn attribute for=VideoTrackReader>\[[track]]</dfn></dt>
  <dd>The {{MediaStreamTrack}} provided at construction.</dd>
  <dt><dfn attribute for=VideoTrackReader>\[[callback]]</dfn></dt>
  <dd>
    The {{VideoFrameOutputCallback}} assigned by the last call to
      {{VideoTrackReader/start()}}.
  </dd>
</dl>


Constructors{#videotrackreader-constructors}
---------------------------------------------
<dfn constructor for=VideoTrackReader title="VideoTrackReader(track)">
  VideoTrackReader(track)
</dfn>
1. If `track.kind` is not `"video"`, throw a {{TypeError}}.
2. If `track.readyState` is `"ended"`, throw an {{InvalidStateError}}.
3. Let |reader| be a new {{VideoTrackReader}} object.
4. Assign <var ignore>track</var> to the {{VideoTrackReader/[[track]]}} internal slot.
5. Assign `"stopped"` to `reader.readyState`.
6. Return |reader|.


Attributes{#videotrackreader-attributes}
----------------------------------------
<dl>
  <dt><dfn attribute for=VideoTrackReader>readyState</dfn></dt>
  <dd>
    Indicates the current state of the {{VideoTrackReader}} object.
  </dd>
  <dt><dfn attribute for=VideoTrackReader>onended</dfn></dt>
  <dd>The event handler for the ended event.</dd>
</dl>

Event Summary{#videotrackreader-events}
---------------------------------------
<dl>
  <dt><dfn event for=VideoTrackReader>ended</dfn></dt>
  <dd>
    Dispatched when the {{VideoTrackReader/[[track]]}}'s
        {{VideoTrackReader/readyState}} becomes `"ended"`, indicating no further
        {{VideoFrame}}s will be output.
  </dd>
</dl>


Methods{#videotrackreader-methods}
----------------------------------
<dfn method for=VideoTrackReader>start(callback)</dfn>

Starts calling the callback with {{VideoFrame}}s from the {{MediaStreamTrack}}.

When invoked, run these steps:
1. If {{VideoTrackReader/readyState}} is not `"stopped"`, throw an
    {{InvalidStateError}}.
2. Assign `"started"` to {{VideoTrackReader/readyState}}.
3. Assign <var ignore>callback</var> to the {{VideoTrackReader/[[callback]]}} internal slot.
4. In parallel, run the <a>track monitor</a>.

<dfn method for=VideoTrackReader>stop()</dfn>

Stops calling the {{VideoFrameOutputCallback}} with {{VideoFrame}}s from the
    {{MediaStreamTrack}}.

When invoked, run these steps:
1. If readyState is not `"started"`, throw an {{InvalidStateError}}.
2. Cease running the <a>track monitor</a>.
3. Assign `"stopped"` to the {{VideoTrackReader/readyState}}.
4. Assign `null` to the {{VideoTrackReader/[[callback]]}} internal slot.


MediaStreamTrack Monitoring{#mediastreamtrack-monitoring}
---------------------------------------------------------

The <dfn>track monitor</dfn> may be started and stopped by the user to control
the calling of the {{VideoFrameOutputCallback}}.

<dfn>Running the track monitor</dfn> means monitoring
{{VideoTrackReader/[[track]]}} for the arrival of new picture data as well as
changes to `[[track]].readyState`.

While `[[track]].readyState` is "live", for each new picture that arrives in
{{VideoTrackReader/[[track]]}}, execute the following steps:

    NOTE: Pictures that arrived prior to the start of this loop are not
        considered.


    NOTE: Video data in a MediaStreamTrack does not have a canonical binary
        form. The user agent should tokenize "pictures" by discrete times of
        capture. For example, if the source is a camera capturing 60 frames per
        second, the UA should construct 60 corresponding VideoFrame's each
        second.

1. Let |planes| be a sequence of {{Plane}}s containing the picture data.
2. Let |pixelFormat| be the {{PixelFormat}} of |planes|.

    NOTE: This section needs work. The UA should avoid early optimizations to
        convert between PixelFormats, but currently only a narrow set of formats
        is defined in this spec. We should consider adding an "opaque" format
        along with an API coverter API to make pixel format conversion
        transparent to authors.


3. Let |frameInit| be an {{VideoFrameInit}} with the following keys:
    1. Let <var ignore>timestamp</var> and <var ignore>duration</var> be the presentation timestamp and
        (optionally) presentation duration as determined by the
        {{VideoTrackReader/[[track]]}} source.
    2. Let <var ignore>codedWidth</var> and <var ignore>codedHeight> be the width and height of the decoded
        video frame in pixels, prior to any cropping or aspect ratio
        adjustments.
    3. Let <var ignore>cropLeft</var>, <var ignore>cropTop</var>, <var ignore>cropWidth</var>, and <var ignore>cropHeight</var> be the crop
        region of the decoded video frame in pixels, prior to any aspect ratio
        adjustments.
    4. Let <<var ignore>displayWidth</var> and <<var ignore>displayHeight</var> be the display size of the decoded
        video frame in pixels.
4. Let |frame| be a {{VideoFrame}}, constructed with |pixelFormat|,
        |planes|, and |frameInit|.
5. Invoke {{VideoTrackReader/[[callback]]}} with |frame|.

If {{VideoTrackReader/[[track]]}}.readyState becomes `"ended"`,
    <a>queue a task</a> on the media element task source to run the
    following steps:

1. Set {{VideoTrackReader/readyState}} to "ended".
2. <a>Queue a task</a> on the media element task source to run a simple
    event named {{VideoTrackReader/ended}} at the {{VideoTrackReader}}.
